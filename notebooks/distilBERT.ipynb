{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nithe\\Documents\\projects\\Support_Ticket_Auto_Triage\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"Tobi-Bueck/customer-support-tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44c23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbf4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#switching to pandas\n",
    "df = ds['train'].to_pandas()\n",
    "df.info\n",
    "\n",
    "#filtering only to english tickets\n",
    "df_en = df[df['language'] == 'en'].copy()\n",
    "\n",
    "#there are like 50 different labels and so reducing to top 10. Naming the rest of the lables as 'other'\n",
    "top_label=df_en['queue'].value_counts().head(10).index.tolist()\n",
    "df_en['label'] = np.where(df_en['queue'].isin(top_label), df_en['queue'], 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3df96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 22608\n",
      "Valid rows: 2826\n",
      "Test rows:  2827\n"
     ]
    }
   ],
   "source": [
    "#spliting the dataset \n",
    "train, temp = train_test_split(df_en, test_size=0.2, stratify=df_en['label'], random_state=42)\n",
    "valid, test = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n",
    "\n",
    "print(f\"Train rows: {len(train)}\")\n",
    "print(f\"Valid rows: {len(valid)}\")\n",
    "print(f\"Test rows:  {len(test)}\")\n",
    "\n",
    "test['text'] = test['subject'].fillna('')+ '\\n' + test['body'].fillna('')\n",
    "valid['text'] = valid['subject'].fillna('')+ '\\n' + valid['body'].fillna('')\n",
    "train['text'] = train['subject'].fillna('')+ '\\n' + train['body'].fillna('')\n",
    "\n",
    "x_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "x_val = valid['text']\n",
    "y_val = valid['label']\n",
    "\n",
    "x_test = test['text']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76e4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 classes: ['Billing and Payments', 'Customer Service', 'General Inquiry']\n",
      "Tokenizing Train Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22608/22608 [00:01<00:00, 15070.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Validation Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2826/2826 [00:00<00:00, 18778.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Test Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2827/2827 [00:00<00:00, 18639.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ready for Training! Train size: 22608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "#get unique labels from trining data\n",
    "unique_labels = sorted(list(set(y_train)))\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Found {len(unique_labels)} classes: {unique_labels[:3]}\")\n",
    "\n",
    "#helper functions to tokenize and format\n",
    "def process_data(texts, labels):\n",
    "    #convert text series to list\n",
    "    texts = texts.tolist()\n",
    "    #map labels to integers\n",
    "    label_ids = [label2id[label] for label in labels]\n",
    "    \n",
    "    #create dataset object\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': texts,\n",
    "        'label': label_ids\n",
    "    })\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=128),\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    #format for PyTorch \n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    return dataset\n",
    "\n",
    "#process all splits\n",
    "print(\"Tokenizing Train Data...\")\n",
    "train_dataset = process_data(x_train, y_train)\n",
    "\n",
    "print(\"Tokenizing Validation Data...\")\n",
    "val_dataset = process_data(x_val, y_val)\n",
    "\n",
    "print(\"Tokenizing Test Data...\")\n",
    "test_dataset = process_data(x_test, y_test)\n",
    "\n",
    "print(f\"\\nReady for Training! Train size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights Calculated: tensor([0.9753, 0.6620, 6.9994], device='cuda:0')...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Weighted Training (10 Epochs)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14130' max='14130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14130/14130 20:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.876100</td>\n",
       "      <td>1.758638</td>\n",
       "      <td>0.337933</td>\n",
       "      <td>0.344685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.387700</td>\n",
       "      <td>1.620335</td>\n",
       "      <td>0.390658</td>\n",
       "      <td>0.403866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.276400</td>\n",
       "      <td>1.484128</td>\n",
       "      <td>0.441260</td>\n",
       "      <td>0.460032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>1.437346</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>0.524875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>1.387214</td>\n",
       "      <td>0.533616</td>\n",
       "      <td>0.577642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>1.391987</td>\n",
       "      <td>0.587757</td>\n",
       "      <td>0.632031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>1.442454</td>\n",
       "      <td>0.626327</td>\n",
       "      <td>0.665753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>1.434248</td>\n",
       "      <td>0.644728</td>\n",
       "      <td>0.677058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>1.472113</td>\n",
       "      <td>0.658174</td>\n",
       "      <td>0.691705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>1.477622</td>\n",
       "      <td>0.670205</td>\n",
       "      <td>0.700828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14130, training_loss=0.849076042458763, metrics={'train_runtime': 1250.5511, 'train_samples_per_second': 180.784, 'train_steps_per_second': 11.299, 'total_flos': 7488125540352000.0, 'train_loss': 0.849076042458763, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"Class Weights Calculated: {weights_tensor[:3]}...\") \n",
    "\n",
    "#different weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_weighted',\n",
    "    num_train_epochs=10,             \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,              \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "#initialize model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "#initialize trainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting Weighted Training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a18f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation on Test Set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TEST REPORT\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "           Billing and Payments       0.88      0.84      0.86       290\n",
      "               Customer Service       0.62      0.66      0.64       427\n",
      "                General Inquiry       0.88      0.72      0.79        40\n",
      "                Human Resources       0.79      0.80      0.79        55\n",
      "                     IT Support       0.62      0.72      0.67       334\n",
      "                Product Support       0.64      0.62      0.63       531\n",
      "          Returns and Exchanges       0.76      0.69      0.73       140\n",
      "            Sales and Pre-Sales       0.76      0.63      0.69        84\n",
      "Service Outages and Maintenance       0.84      0.87      0.85       111\n",
      "              Technical Support       0.71      0.68      0.70       815\n",
      "\n",
      "                       accuracy                           0.70      2827\n",
      "                      macro avg       0.75      0.73      0.74      2827\n",
      "                   weighted avg       0.70      0.70      0.70      2827\n",
      "\n",
      "Model saved to ../models/transformer\n"
     ]
    }
   ],
   "source": [
    "print(\"Running final evaluation on Test Set...\")\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "# convert raw scores to labels\n",
    "y_test_preds = np.argmax(test_output.predictions, axis=1)\n",
    "y_test_labels = test_output.label_ids\n",
    "\n",
    "print(\"\\nFINAL TEST REPORT\")\n",
    "print(classification_report(y_test_labels, y_test_preds, target_names=unique_labels))\n",
    "\n",
    "save_path = \"../models/transformer\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "support-ticket-auto-triage (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
